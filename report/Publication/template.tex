%%%%%%%%%%%%%%%%%%%% author.tex %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% sample root file for your "contribution" to a contributed volume
%
% Use this file as a template for your own input.
%
%%%%%%%%%%%%%%%% Springer %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% RECOMMENDED %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[graybox]{svmult}

% choose options for [] as required from the list
% in the Reference Guide
\usepackage{comment}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{graphicx,}
\usepackage{amsmath}
\usepackage[tight]{subfigure}
\usepackage{mathptmx}       % selects Times Roman as basic font
\usepackage{helvet}         % selects Helvetica as sans-serif font
\usepackage{courier}        % selects Courier as typewriter font
\usepackage{type1cm}        % activate if the above 3 fonts are
                            % not available on your system
%
\usepackage{makeidx}         % allows index generation
\usepackage{graphicx}        % standard LaTeX graphics tool
                             % when including figure files
\usepackage{multicol}        % used for the two-column index
\usepackage[bottom]{footmisc}% places footnotes at page bottom
\bibliographystyle{ieeetr}
\usepackage{algorithm2e}	% For algorithms
 \usepackage{float}
% see the list of further useful packages
% in the Reference Guide
\makeindex             % used for the subject index
                       % please use the style svind.ist with
                       % your makeindex program

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Commands: To make life easier
\newcommand{\inreal}{\in \mathbb{R}}
\newcommand{\inrealmxn}{\in \mathbb{R}^{m\times n}}
\newcommand{\inints}{\in \mathbb{Z}}
\newcommand{\innatural}{\in \mathbb{N}}
\newcommand{\smat}{\mathbf{S}}
\newcommand{\covmat}{\mathbf{A}}
\newcommand{\xvec}{\mathbf{x}}
\newcommand{\vvec}{\mathbf{v}}
\newcommand{\vmat}{\mathbf{V}}
\newcommand{\tp}{^T}
\DeclareMathOperator{\Tr}{Tr}
\newcommand{\new}{_\text{new}}
\newcommand{\old}{_\text{old}}
\newcommand{\BigO}[1]{\ensuremath{\operatorname{O}\left(#1\right)}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\title*{Improving Sparse PCA Results in the Context of London Twitter Data}
% Use \titlerunning{Short Title} for an abbreviated version of
% your contribution title if the original one is too long
\author{Theo Pavlakou \and Arta Babaee \and Moez Draief}
%TODO Wouldn't let me add this for some reason.
% Use \authorrunning{Short Title} for an abbreviated version of
% your contribution title if the original one is too long
%\institute{Arta Babaee \at Intelligent Systems and Networks Group, Department of Electrical and Electronic Engineering, \\
%Imperial College London, London, UK\\
%\email{ab3608@imperial.ac.uk}
%\and Moez Draief \\
%\email{mmd@imperial.ac.uk}}
%
% Use the package "url.sty" to avoid
% problems with special characters
% used in your e-mail or web address
%
\maketitle


\abstract{
%TODO Change this
Motivated by the vast amounts of work currently being done in Big Data analysis using social media and the advancements in algorithms to calculate sparse principal components it is proposed that the performance of these algorithms can be improved by altering these matrices taking into account their underlying structure. Using Twitter data from London in the year 2012 as the medium to demonstrate on, various alterations are made to the matrix obtained and the resulting matrices are then passed through a specific sparse principal component analysis algorithm to give different outputs. These are then analysed and it is shown that indeed the results do differ with one particular variation consistently outperforming the rest. These results are especially of interest when the data to be analysed can be represented by a binary matrix of some sort, such as in document analysis. 
}

\section{INTRODUCTION}\label{intro}
Principal Component Analysis (PCA) is a mathematical technique that is concerned with finding linear combinations of the features in a set of features which lead to the highest variation in outcomes, which in turn means leading to the highest information gain. The problem with PCA is that it can be hard to interpret since the size of the vector that is returned could be immense in length, with only a fraction of the entries being of significance. This has led to the creation of algorithms that return sparse representations approximating the principal components, but which lead to results that are much easier to interpret, which will be referred to as Sparse PCA algorithms in this paper. 

Much work has been done in this area \cite{shen}, \cite{daspremont}, \cite{zou}, \cite{truncpower} and plenty others have created or improved existing algorithms to retrieve sparse principal component like vectors. This paper aims to focus on binary data and show that with knowledge of what the data represents beforehand, the data can be preprocessed in such a way as to improve the output of these algorithms. The data used is a batch of Tweets from London throughout the year 2012\footnote{This is the same data as that used in \cite{microblogs}}, which is parsed and turned into a binary matrix, as is done in \cite{dimakis} and the same algorithm is used, to be able to fairly compare results. The difference lies in the fact that the matrix to be given as an input to the algorithm will be slightly changed and will attempt to give reasons as to why the resulting matrix is a better choice as an input, for binary data matrices, with backing from resulting experiments using real data. 

The structure of this paper is as follows: in section \ref{algorithm} the sparse principal component analysis algorithm that is used for the data analysis is explained. In section \ref{improvement} various modifications are made to improve the algorithm's performance, testing is done on real Twitter data and the results are analysed. Finally, in section \ref{conclusion} the results and analysis are discussed and further work is proposed also. 

\section{Notation}

Throughout the remaining sections, the following mathematical notations will be used. $\mathbf{S} \in \mathbb{Z^*}^{m\times n}$ is the data matrix consisting of $m$ data points (in the specific case considered in depth in this paper, the Tweets) evaluated on $n$ features (words), with $[\mathbf{S}]_{i,j} = 1$ when Tweet $i$ contains word $j$ and zero otherwise. Likewise, the variable $m$ is reserved for the number of Tweets and $n$ for the number of words. $\mathbf{A} \in \mathbb{Z^*}^{n \times n}$ is equal to $\mathbf{S}^T\mathbf{S}$ and this corresponds to the co-occurence matrix of $\mathbf{S}$. $\mathbf{v}_i$ is to be used for the eigenvector with the $i$th largest eigenvalue, correspondingly denoted by $\lambda_i$. Both $\|\mathbf{x}\|$ and $\|\mathbf{x}\|_2$ denote the $l_2$ norm of a vector, defined as $\sqrt{\mathbf{x}^T\mathbf{x}}$ and $\|\mathbf{x}\|_0$ denotes the $l_0$ norm of a vector, defined as the number of non-zero elements in it. $[\mathbf{x} ]_i$ denotes the $i$th element in a vector and similarly $[\mathbf{A}]_{i, j}$ denotes the element specified by the $i$th row and the $j$th column. For a matrix, $\|\mathbf{A}\|_F$ defines the Frobenius norm, defined as $\sqrt{\sum^m_{i=1}{\sum_{j=0}^n{\mathbf{[A}]_{i, j}^2}}}$.

\section{Sparse PCA through Low-Rank Approximation Algorithm}\label{algorithm}

Sparse PCA algorithms aim to deliver similar results to PCA, but with the difference that they give rise to principal components that only have few non-zero entries. The reason sparsity is desired in the eigenvectors is because sparse approximations of the eigenvectors are useful for interpreting data that have large dimensions. For example, in the specific application discussed in this paper, a set of words is considered and meaningful inferences are to be found from this data. It is clearly much simpler to analyse data given a small set of words that could approximate the variance in the given data set per principal component, than all the words in the data set which would contribute a small amount each, increasing the dimensionality of the principal component to the number of words in the set of all the words in all the Tweets. The equation to be solved resolves to:
\begin{equation}
\mathbf{v}_1 = \underset{\|\mathbf{x}\|_2^2 = 1, \|\mathbf{x}\|_0 = k}{\operatorname{argmax}}\left( \mathbf{x}^T\mathbf{A}\mathbf{x}\right)
\end{equation}
where $\|\mathbf{x}\|_0^2$ is the $l_0$ cardinality of $\mathbf{x}$ i.e. how many non-zero elements $\mathbf{x}$ has. This is where Sparse PCA algorithms come in.

The algorithm used here to demonstrate the results is borrowed from \cite{dimakis} and is explained briefly in the following sections. It has been chosen as the authors of \cite{dimakis} use Twitter data to demonstrate its effectiveness and so, taking into account their results also, it provides a fair means for comparison. Furthermore, it has a safe feature elimination procedure which results in a significant improvement to the computation time and is based on the Spannogram Algorithm described in this section. Added to this, the principal components found are also orthogonal to each other, which is a result of using the projection deflation method described in \cite{Mackey_deflationmethods} and the sparsity of the eigenvector can be stated beforehand unlike some other methods, for example \cite{GPower}.

\subsection{Explanation of the Algorithm}
The assumption is made that a rank $d$ approximation of the co-occurence matrix, $\covmat_d$, is arbitrarily close to the original based on the Forbenius norm. That is:

\begin{equation*}
\|\mathbf{A} - \mathbf{A}_d\|_F < \epsilon
\end{equation*}
where $\epsilon$ can be determined. This can then be expressed as:
\begin{equation*}
\mathbf{A}_d = \sum_{i=1}^d \lambda_i \mathbf{v}_i \mathbf{v}_i^T
\end{equation*}

The basic principal behind this algorithm is that the sparse approximation to the first principal component can be found by calculating the eigenvector $\mathbf{x}_*$ with the largest eigenvalue for all the possible $k\times k$ submatrices of $\mathbf{A}$. This would lead to finding $n \choose k$ possible sparse eigenvectors, however, which would be quite wasteful since it would lead to needing to find $O\left( n^k\right)$ eigenvectors, where $k$ is the sparsity of the eigenvector, i.e. how many elements of the vector are non-zero. In \cite{dimakis}, it can be shown that only $\BigO {n^d}$ candidates need to be examined though and since $d < k$ typically, this means that the time complexity is reduced significantly. This reduction is the result of the Spannogram algorithm which is also described by the authors and the resulting time complexity turns out to be $\BigO { n^{d+1}\log(n) + n^d k^2 + dn^2}$. Depending on the structure of the matrix $\covmat_d$, either one of the terms could prevail.

\subsection{The Spannogram Algorithm}

What this algorithm aims to do is eliminate any candidates for the eigenvectors that would be redundant to calculate. The way this can be demonstrated is by taking a Rank-1 matrix, i.e.

\begin{equation*}
\mathbf{A}_1 = \lambda_1 \mathbf{v}_1 \mathbf{v}_1^T
\end{equation*}

and solving the equation

\begin{equation*}
 \underset{\|\mathbf{x}\|_2^2 = 1, \|\mathbf{x}\|_0 = k, \mathbf{x} \in\mathbb{S}_k} \max( \mathbf{x}^T\mathbf{A}_1\mathbf{x})
\end{equation*}

where $\mathbb{S}_k$ is the set of all $k$-sparse vectors in $\mathbb{Z^*}^n$. The solution turns out to be:

\begin{equation*}
\underset{\|\mathbf{x}\|_2^2 = 1, \|\mathbf{x}\|_0 = k, \mathbf{x} \in\mathbb{S}_k} \max\lambda_1 \left(\mathbf{v}_1^T \mathbf{x}\right)^2 
= \underset{\|\mathbf{x}\|_2^2 = 1, \|\mathbf{x}\|_0 = k, \mathbf{x} \in\mathbb{S}_k} \max \lambda_1\left( \sum_{i = 1}^n [\mathbf{v}_{1}]_i [\mathbf{x}]_i\right)^2
\end{equation*}

Therefore, it can easily be seen that just by keeping the $k$ entries with the largest magnitude and making all other entries equal to 0 in x would satisfy the equation above and finding the solution turns out to be trivial. 

For the case where $d > 1$, what the algorithm essentially does is transform the problem into many Rank-1 case problems and solve them to find the best solution.

The eigenvectors of $\mathbf{A}_d$, which can be represented as:
\begin{equation*}
\mathbf{V}_d = [\sqrt{\lambda_1}\mathbf{v}_1, \cdots, \sqrt{\lambda_d}\mathbf{v}_d]
\end{equation*}

\begin{equation*}
\underset{\|\mathbf{x}\|_2^2 = 1, \|\mathbf{x}\|_0 = k, \mathbf{x} \in\mathbb{S}_k} \max  \mathbf{x}^T\mathbf{A}_d\mathbf{x} 
= \underset{\|\mathbf{x}\|_2^2 = 1, \|\mathbf{x}\|_0 = k, \mathbf{x} \in\mathbb{S}_k}\max\|\mathbf{V}_d^T\mathbf{x}\|_2^2
\end{equation*}


It then defines a vector, $\mathbf{c} \in \mathbb{R}^d$ such that $\|\mathbf{c}\|_2=1$, which is a function of some vector $\mathbf{\phi}$ and the elements in it are sinusoids taking as arguments the elements in $\mathbf{\phi}$, where each $\phi_i \in (\frac{-\pi}{2}, \frac{\pi}{2}]$.

The equation is then transformed into a Rank-1 instance by creating the vector $\mathbf{u}_c = \mathbf{u}_d \mathbf{c}$ to give 

\begin{equation}
\max_{\|\mathbf{c}\|_2 =1} \left(\underset{\|\mathbf{x}\|_2^2 = 1, \|\mathbf{x}\|_0 = k, \mathbf{x} \in\mathbb{S}_k}\max\left(\mathbf{v}_c^T \mathbf{x}\right)^2 \right)
\label{spannogram}
\end{equation}

Therefore, when $\mathbf{c}$ is fixed there is a simple trivial solution to  equation \ref{spannogram}. The important thing to note is that, there is no actual need to calculate the actual value of the $\mathbf{v}_c$ for each $\phi$ to find the candidate indices for the $k$-sparse eigenvectors only the relative magnitudes between the different entries in  $\mathbf{v}_c$ must be taken into account. The relative magnitudes will only change at intersection points between the sinusoids, and therefore, only about these points must a candidate eigenvector indices actually be tested. This brings down the computation to $O \left(n^d\right)$ instead of $O \left(n^k\right)$. A more thorough description and justification of the algorithm can be found in \cite{dimakis}.


\section{Improving Performance of the Spannogram PCA Algorithm}
\label{improvement}
\subsection{The Data Matrix: $\smat$}
Let the matrix created by the parser be $\mathbf{S} \in \mathbb{Z^*}^{m \times n}$, where $m$ is the number of tweets and $n$ is the number of words in the bag of words. Typically, it is expected that in this application $m >> n$, i.e. there are many more data points than the dimensionality of the data. For the matrices considered in this paper, $m \approx 40,000$ and $n = 3000$. This is created almost identically to that in \cite{dimakis} i.e. words with less than 3 characters are omitted, common words such as ``him'' and ``that'' which convey little to no information are also filtered out, etc.

\subsection{The Co-occurrence Matrix: $\mathbf{A}$}
\label{covmat}

This is the matrix that is of interest in this paper as it determines what sort of relationship the words appear to have with one another. This can be seen since the initial matrix is given by $\covmat = \smat\tp\smat$ and can be viewed as the weighted adjacency matrix of an undirected graph with each vertex being a word and each edge weight being the total number of times that word appears with the word represented by the connecting vertex over all the Tweets. The element $[\covmat]_{i, j}$ is the total number of times word $i$ appears with the word $j$ in the set of Tweets being considered and any number on the diagonal, $[\covmat]_{i, i}$, is the total number of times word $i$ appears. 

In this section, variations of the matrix are considered and evaluated against each other. All testing is done using a subset of the Tweets, namely on batches from the following periods: from the 23rd September 2012 to the 24th September 2012, from the 25th September 2012 to the 26th September 2012, and from the 30th September 2012 to the 1st October 2012, roughly a day's worth of data, and this gives about 40,000 Tweets ($m$) and 3000 words ($n$). The Spannogram Algorithm is run on each of the matrices using the same parameters in each case\footnote{Using a rank 2 approximation} and the first 8-sparse PC is computed.

The algorithm is tried on 4 variants of the actual co-occurrence matrix and the results are then analysed. 


\subsubsection{The Initial Matrix: $\mathbf{A}$}
Initially, the approach considered in \cite{dimakis} is taken, in which $\mathbf{A} = \mathbf{S}^T \mathbf{S}$.


  
\subsubsection{The Hollow Matrix: $\mathbf{A}_{h}$}

In the initial matrix the diagonal represents the number of times each word appears in total, regardless of any relation to the other words i.e. each node has a link to itself, with the highest weighting. This means that words that occur very frequently have very large values on their diagonal, regardless of how they relate to other words, which can be deceiving when taking the eigenvalues of the matrix, since words that may not have been filtered out of the bag of words, such as ``haha'' or ``this'', may appear very frequently but not have a distinct pattern as to the sorts of words they pair with most frequently. Since the whole point of the sparse principal components is to find a relationship of a few words together, it is only natural to want to prevent this. The matrix $\mathbf{A}$, therefore, is substituted for 
$\mathbf{A}_h$ where for each of its elements, $[\mathbf{A}_h]_{i, j}$, it takes the value 

\begin{equation}
[\mathbf{A}_h]_{i, j}= 
\begin{cases}
[\mathbf{A}]_{i, j} & \text{if}\ i \neq j\\
0 & \text{if}\ i = j
\end{cases}
\end{equation}
\begin{comment}
In this case, the second sparse PC once again does not represent any significant occasion and seems to just be a few words that may occur frequently together or separately. Contrary to the previous case though, the first sparse PC refers to a very specific event regarding a murder that took place on the 19th September 2012, where 2 police officers were killed and which resulted in a trending topic on Twitter. 

What is useful to note is that in the case of the first PC 7 of the 8 words rank much lower than the top 20 words, the lowest one occurring merely 34 times and the highest appearing 485 times, which is comparable to the lowest in the previous case. This clearly highlights the positive effect of removing the link of the each vertex to itself, which allows the algorithm to discover relationships which would otherwise be completely overpowered. 

\end{comment}

\subsubsection{The Weighted Laplacian Matrix: $\mathbf{A}_{l}$}

The Laplacian of a graph is a very useful representation of it and is very useful in applications involving Spectral Graph Theory and Partial Differential Equations. $\mathbf{A}_{l}$ of a graph is given by the following equation:

\begin{equation}
\mathbf{A}_{l} = \mathbf{D} - \mathbf{B}
\end{equation}

Here $\mathbf{D}$ is the degree matrix of the graph, which is a diagonal matrix and each element $j$ on the diagonal is the sum of the weights of the edges that are connected to the vertex representing word $j$. $\mathbf{B}$ is the weighted adjacency matrix and is equal to $\smat\tp\smat$. More about the Laplacian is explained in \cite{laplacian_spielman}. 

\subsubsection{The Weighted Laplacian of the Hollow Matrix: $\mathbf{A}_{lh}$}

This time the Laplacian matrix is created using the graph represented by the hollow matrix, $\covmat_h$ described above, i.e. there are no edges from a word to itself.

\subsection{Results}

The results can be seen in Tables \ref{pcs_jterry}, \ref{pcs_police} and \ref{pcs_ryder}. As it can be seen, only the hollow matrix gives PCs which corresponds to an actual single distinct event that took place at the times considered. The one in Table \ref{pcs_jterry} refers the retirement of England's former captain, John Terry, from international football\footnote{Confirmed by the Guardian: http://www.theguardian.com/football/2012/sep/23/john-terry-retires-international-football}, the one in Table \ref{pcs_police} is regarding a murder that took place on the 19th September 2012, in which 2 police officers were killed \footnote{Confirmed by the BBC: http://www.bbc.co.uk/news/uk-england-19634164} and which resulted in a trending topic on Twitter and the one in Table \ref{pcs_ryder} emerges as a result of Europe's victory over the US in the Ryder Cup golf competition in 2012. \footnote{Confirmed by the BBC: http://www.bbc.co.uk/sport/0/golf/19780678}

\begin{comment}
The final principal component, visible in Table \ref{ryder_cup}, 
These are the most interesting principal components that appear from running the algorithm on the data, however some others also come up, which are associated with advertisements. Some batches, contrariwise, did not have any relevant principal components, which is a result of there not being any significant event that took place during the associated time periods.
\end{comment}

\begin{table}[H]
\center
\begin{tabular}{| r | l | r | l| r | l | r | l|}
\hline
\multicolumn{2}{|c|}{Matrix $\covmat$ }& \multicolumn{2}{|c|}{Matrix $\covmat_h$}& \multicolumn{2}{|c|}{Matrix $\covmat_l$} & \multicolumn{2}{|c|}{Matrix $\covmat_{lh}$} \\

\hline
Index & Word &Index & Word & Index & Word & Index & Word\\
\hline
1 & haha & 121 & \textbf{terry} & 12 & them& 5 & still\\
7 & yeah  & 120 & \textbf{john} &68 & their &  3 & night\\
3 & night&321 & \textbf{international}& 10 & great  & 7 & yeah \\

5 & still & 908 & \textbf{retires}&44 & ever  & 12 & them \\

12 & them& 772 & \textbf{retired}&33 & better   & 2 & need\\

6 & work& 1558 & \textbf{retirement}  &3& night & 11 & only  \\ 

2 & need &66 & \textbf{football} & 29 & after & 6 & work\\
 
9 & thanks& 144 & \textbf{england}  & 21 & weak & 33 & better\\

\hline
\end{tabular}
\label{pcs_jterry}
\caption{The first principal component of Twitter data during the period 23rd September 2012 to the 24th September 2012 using the Spannogram Algorithm on the variants of the co-occurrence matrix. The index represents the rank of the word according to how frequent the word is found in the set of these Tweets. The bold words show words that do seem to be related according to events that have been confirmed.}
\end{table}



\begin{table}[H]
\center
\begin{tabular}{| r | l | r | l| r | l | r | l|}
\hline
\multicolumn{2}{|c|}{Matrix $\covmat$ }& \multicolumn{2}{|c|}{Matrix $\covmat_h$}& \multicolumn{2}{|c|}{Matrix $\covmat_l$} & \multicolumn{2}{|c|}{Matrix $\covmat_{lh}$} \\

\hline
Index & Word &Index & Word & Index & Word & Index & Word\\
\hline
2 & need & 12 & \textbf{please}  & 6 & them & 6 & them\\
1 & haha & 827 & \textbf{officers}  &28 & over &  4 & work\\
4 & work &889 & \textbf{murders} & 31 & down& 9 & only \\

3 & want & 240 & \textbf{following} &32 & after & 2 & need \\

6 & them & 783 & \textbf{fallen}&13 & make  & 13 & make\\

10 & still & 756 & \textbf{recent} &42 & week & 10 & still  \\ 

7 & yeah&190 & \textbf{police}  & 43 & though & 31 & down\\
 
9 & only & 787 & \textbf{colleagues} & 21 & look & 67 & something \\


\hline
\end{tabular}
\label{pcs_police}
\caption{The first principal component of Twitter data during the period 25th September 2012 to the 26th September 2012 using the Spannogram Algorithm on the variants of the co-occurrence matrix. The index represents the rank of the word according to how frequent the word is found in the set of these Tweets. The bold words show words that do seem to be related according to events that have been confirmed.}
\end{table}

\begin{table}[H]
\center
\begin{tabular}{| r | l | r | l| r | l | r | l|}
\hline
\multicolumn{2}{|c|}{Matrix $\covmat$ }& \multicolumn{2}{|c|}{Matrix $\covmat_h$}& \multicolumn{2}{|c|}{Matrix $\covmat_l$} & \multicolumn{2}{|c|}{Matrix $\covmat_{lh}$} \\

\hline
Index & Word &Index & Word & Index & Word & Index & Word\\
\hline
2 & haha & 14 & \textbf{rydercup} & 16 & them& 7 & still\\
4 & need  & 38 & \textbf{europe} & 9 & only &  6 & xfactor\\
8 & work &91 & \textbf{rydercup2012}& 7 & still  & 8 & work \\

5 & work & 160 & \textbf{teameurope}&44 & year  & 27 & right \\

7 & still &  12 & \textbf{great}&27 & right   & 40 & life\\

6 & xfactor & 77 & \textbf{golf}  &64& weekend & 112 & other  \\ 

3 & night &23 & done & 37 & week & 162 & there's\\
 
9 & only & 102 & \textbf{ryder}  & 8 & work & 9 & only\\

\hline
\end{tabular}
\label{pcs_ryder}
\caption{The first principal component of Twitter data during the period 30th September 2012 to the 1st October 2012 using the Spannogram Algorithm on the variants of the co-occurrence matrix. The index represents the rank of the word according to how frequent the word is found in the set of these Tweets. The bold words show words that do seem to be related according to events that have been confirmed.}
\end{table}


What is very useful to note is that in the case of the matrix $\covmat$ in every single occasion the words that appear are in the top 20 most frequently used words and they do not really convey much information as to any particular event that took place in the periods considered. Essentially, the algorithm seems to be choosing these words purely because they occur very frequently, for instance, the word that is chosen for the matrix $\covmat$ that has the highest ranking index (and therefore the lowest frequency) in Table \ref{pcs_ryder} is ``only'', which appears 541 times. Similar results can be seen for the matrices $\covmat_l$ and $\covmat_{lh}$, though not as radical, and are all consequences of the fact that the diagonal always overpowers the off-diagonal elements in the resulting matrices. On the contrary, for the case of the hollow matrix, the lowest frequency word is ``teameurope'', which appears merely 160 times and for other PCs this is even lower (``retirement'' has an index of 1558). 

\section{Conclusion}\label{conclusion}
%TODO Build on this section more

It has been confirmed that using Sparse PCA algorithms it is possible to infer results from microblogs such as Twitter, as is done both in \cite{dimakis}. Different matrices representing the data features have then been analysed and inputted to the algorithm for Sparse PCA to produce results. These have shown that the hollow matrix, $\covmat_h$, gives the best results by far when comparing the the resulting principal components returned for each of the inputs. Moreover, though the Twitter Data is the same as that used in \cite{microblogs}, the events that are discovered are different but still completely valid as can be confirmed by other sources, which highlights the fact that this approach may be well suited in conjunction to the methods described there. The choice of this matrix, as opposed to the one used in \cite{dimakis}, can be applied to any sparse principal component analysis application when using binary data to discover relationships in features that may otherwise be overpowered by components that do not necessarily give the best results. 

\acknowledgement

Moez Draief is supported by QNRF through NPRP grant number 09-1150-2-148. 
%TODO Add Glasgow 




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliography{bibliography}{}




\end{document}
