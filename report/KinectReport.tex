\documentclass[11pt,a4paper]{article}
%\textheight = 630pt
%\textwidth = 480pt
%\topmargin = 3pt
%\voffset = 0pt
%\headsep = 2pt
%\headheight = 1pt
%\oddsidemargin = 1pt
%\marginparwidth = 1pt

%% Save space packages and settings %%
\usepackage{cite}

\usepackage{paralist}
\usepackage[compact]{titlesec}
\titlespacing{\section}{0pt}{0.5ex}{0.5ex}
\titlespacing{\subsection}{0pt}{0.5ex}{0ex}
\titlespacing{\subsubseSction}{0pt}{0.5ex}{0ex}
\linespread{0.9}

\usepackage{changepage}
\usepackage[hmargin=2cm, vmargin=2cm]{geometry}
\usepackage{parskip}
\setlength{\parskip}{5pt}
\usepackage{fancyvrb}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{capt-of}
\usepackage{amsfonts}
\usepackage{verbatim}
\usepackage{courier}
\usepackage{float}
\restylefloat{table}

%%%%%%%%%%%%%%%%%%% Code %%%%%%%%%%%%%%%%%%%%%
\usepackage{color}
\usepackage[table]{xcolor} %adding background color to your tables
\usepackage{listings}% Allows you to present C++ syntax as it looks
\usepackage{listings} %enables inputing code set the settings below
\definecolor{dkgreen}{rgb}{0,0.45,0}
\definecolor{gray}{rgb}{0.2,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}
%\definecolor{purple}{RGB}}{204, 45, 109}
\lstset{ %
language=C, % choose the language of the code
commentstyle=\color{dkgreen},
basicstyle=\footnotesize\ttfamily, % the size of the fonts that are used for the code
numbers=left, % where to put the line-numbers
numberstyle=\footnotesize, % the size of the fonts that are used for the line-numbers
stepnumber=1, % the step between two line-numbers. If it is 1 each line will be numbered
numbersep=5pt, % how far the line-numbers are from the code
backgroundcolor=\color{white}, % choose the background color. You must add \usepackage{color}
showspaces=false, % show spaces adding particular underscores
showstringspaces=false, % underline spaces within strings
showtabs=false, % show tabs within strings adding particular underscores
frame=single, % adds a frame around the code
tabsize=2, % sets default tabsize to 2 spaces
captionpos=b, % sets the caption-position to bottom
breaklines=true, % sets automatic line breaking
breakatwhitespace=false, % sets if automatic breaks should only happen at whitespace
keywordstyle=\color{purple}, % keyword style
numberstyle=\tiny\color{gray}, % the style that is used for the line-numbers
rulecolor=\color{black}, % if not set, the frame-color may be changed on
stringstyle=\color{blue}, % string literal style
escapeinside={\%*}{*)} % if you want to add a comment within your code
}
\DefineVerbatimEnvironment{code}{Verbatim}{fontsize=\small}
\DefineVerbatimEnvironment{example}{Verbatim}{fontsize=\small}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\title{Analysing London Twitter Data using Sparse PCA}
\author{
Theo Pavlakou\\ \texttt{tp710}\\ CID 00651782\\
}
\date{\today}
\maketitle
\pagenumbering{gobble}
\newpage


\section*{\center Abstract}

\clearpage
\pagenumbering{arabic}

\section{Introduction}
Social media now shapes our lives in such a vast way. More than \textbf{(Find percentage and cite)} of people use social media, such as Facebook, Twitter and Instagram on a daily basis, and most use them more than \textbf{(Find number and cite)} times a day. Recently there has been a lot of work that has gone into analysing this vast amount of data that is available freely on the internet to understand trends in populations and to be able to target particular groups for marketing purposes. Lots of money goes into this yearly, and companies such as Google and Facebook spend \textbf{(Find number and cite)} per year on such methods to maintain their competitive edge in industry. 

The purpose of this study is to examine how a certain implementation of a Sparse PCA algorithm, descibed in \cite{dimakis}, can help to infer results from social media. The data used is a batch of over 48,000 Tweets from London throughout the year 2012. The reason Twitter data has been chosen is due to the word limit, which means that each Tweet is typically quite information dense, which is best to test the algorithm and come up with useful results in the first iteration. Later imporvements may be used on lengthier texts, but a more sophisticated parser may need to be used in that scenario. After concluding meaningful results for the data as a whole, windowing the data into months and then weeks is also attempted and an evolution of the results is examined further to see how the principal components correlate to events that happened in 2012. 

\textbf{NOT FINISHED}

\section{Related Work}
\section{Principal Component Analysis (PCA)}
\subsection{Description}
Principal Component Analysis is a method that can be used to reduce the dimensionality of a data set by projecting it onto the principal subspace that is spanned by the eigenvectors with the largest eigenvalues of the covariance matrix. The reason that the largest eigenvalue eigenvectors are chosen is because these are the eigenvectors along which the largest variance occurs and therefore the most information can be retrieved from. The first principal component (i.e. the first eigenvector) of an $n\times n$ matrix, $A$ is:
\begin{equation}
\underset{\|x\|^2 = 1}{\operatorname{argmax}}\left( x^TAx\right)
\end{equation}
Where $A = S^TS$ and $S$ is the $m\times n$ data set consisting of $m$ data points, each evaluated on $n$ features. \textbf{Explain what $A = S^TS$ is possibly.}
\subsection{Sparse PCA}
\textbf{NOT EXPLAINED WELL - MUST EXPLAIN PROBLEM BETTER}

Sparse PCA is a variant of PCA, which aims to solve the problems associated with performing PCA on datasets that are very sparse i.e. do not have many non-zero entries. For instance, take the dataset that is to be used in this study. There are over 48,000 data points (the Tweets) and, in the best case scenario, roughly 2000 features (the set of words to be evaluated against). This gives a matrix $A$ of dimension $2000\times 2000$. The problem with this is that it is a lot of computation, much of which could be approximated since we know that each row of the matrix will be very sparse. The maximum number of elements that could possibly be non-zero in each row is 140 (since there is a 140 word limit per Tweet) giving a total of 140 non-zero elements of 2000, a mere $7\%$ of the total entries and this is wildly overestimated. It can therefore be seen that the matrix will be very sparse. Therefore the equation to be solved resolves to:
\begin{equation}
\underset{\|x\|_2^2 = 1, \|x\|_0^2 = k}{\operatorname{argmax}}\left( x^TAx\right)
\end{equation}
where $\|x\|_0^2$ is the $l_0$ cardinality of $x$ i.e. how many non-zero elements x has. This is where the Sparse PCA algorithm comes in, which can be found in \cite{dimakis}.


\section{The Bag-of-Words}
The choice of words to be chosen as the features must be carefully considered. For instance, one could simply take a union of all the words in the English dictionary, the words that are used on the internet, such as ``lol'' and ``yey'', and all the possible names of people and places, which would solve the problem of possibly missing out any features. This however would result in a matrix of immense dimension and therefore would be impossible to process, at least given any consumer computer. Therefore, some careful selection for the features must be exhibited so as to gain the features that carry the most information whilst still keeping the number of features to a manageable size. Some general guidelines for all methods are that words that typically do not give any additional information e.g. ``is'', ``the'', ``this'', etc. are to be eliminated. Furthermore, the Bag-of-Words should be completely independent of capital letters i.e. ``Road'' and ``road'' are to be considered equal.
\subsection{Using Set of All Words in Tweets}
This first method attempts to use all the words that appear in the Tweets with the filter described above applied. Unfortunately, this results a set of over 78,000 features, which is much to large to process, so this method is not to be used. 
\subsection{Using Set of Three Thousand Most Common Words in the English Language}
\subsubsection{Description}
The next method is to use a set of the 3,000 most common words in the English language as a whole. A problem with this is that, this list typically includes a lot of the very uninformative words, examples of which are given above, and does not include words that are not very common as a whole but, when some sort of event occurs, become very common, especially on social media. Furthermore, it also does not include any names or slang that is used on the internet. To illustrate, if Muse were to have a concert some time in June, it would be expected that a lot of Tweets may have the word ``Muse'' in them and ``concert'' and words like ``wow'' or ``amazing'', all of which are not in the top 3,000 words in the english language. Therefore, these features would be completly missed out and therefore a very valid principal component would possibly be non-existent in the final results.

\subsubsection{Results}
\subsection{Adding Words in Tweets to the Bag-of-Words with a Certain Probability}
\subsubsection{Description}
Another approach is to say that a word is added to the Bag-of-Words with a probability $p$. The intuition behind this is that words that do appear very often most probably will end up in the set, if p is large enough, but of the words that do not appear very often there would be less, giving a much more relevant bag of words and a much smaller set also ($p$ times the size of the original).


\textbf{CHECK THE MATHS HERE}


We assume that all words are independent. Let $T_w$ be the event that we take the word $w$ and $A_{w, i}$ be the event that the word $w$ appears $i$ times and assume the probability of taking a word $w$, given that $w$ appears is $p$. 
\begin{equation*}
P\left( T_w | \mathbf{A_{w, i}}\right) =  \sum_{j=1}^i {i \choose j} p^j\left( 1 - p\right)^{i-j}
\end{equation*}
which is equivalent to 
\begin{equation}
1 - P(\bar{T_w} |  \mathbf{A_{w, i}}) = 1 - (1 - p)^i
\end{equation}
and assume the probability of $w$ appearing is $q$, therefore the probability of $w$ appearing $i$ times is:
\begin{equation}
P\left(A_{w, i}\right) = q^i
\end{equation}
then the probability of taking the word $w$ is:
\begin{equation*}
P(T_w) = \sum_{i=1}^\infty P\left(T_w| \mathbf{A_{w, i}} \right) P( \mathbf{A_{w, i}})
\end{equation*}
\begin{equation}
\sum_{i=0}^\infty P\left(T_w| \mathbf{A_{w, i}} \right) P( \mathbf{A_{w, i}}) =\sum_{i=1}^\infty \left(1 - (1 - p)^i\right) q^i
\end{equation}

We can therefore see that the higher the probability of $w$ appearing and therefore appearing multiple times, the higher the probability of taking $w$. Since popular events will have words associated with them that appear quite often, it can be said that with the right choice of $p$ (approximately 0.1), more informative words will appear in the Bag-of-Words, whilst also reducing the size of the set. 
\subsubsection{Results}
\subsection{Taking $N$ Words of Highest Occurrence}
In this case, a set of words is created as before, but the frequency of each word in the set is also counted and the words are sorted in descending order of occurrence. When this is done, the first $N$ words are taken as the Bag-of-Words. The assumption here is that words that appear very often will be in the principal components, which is not completely justified, but empirically seems to be the case. \textbf{EXPLAIN BETTER.} This makes the computation feasible whilst still not losing a significant amount of information. 

\section{The Matrix $\mathbf{A}$}
\subsection{The Initial Matrix $\mathbf{A}_{h}$}
The matrix which the sparse principal components are extracted from may take many forms. Initially, the approach considered in \cite{dimakis} is taken, in which $\mathbf{A} = \mathbf{S}^T \mathbf{S}$ where $\mathbf{S} \in \mathbb{R}^{m \times n}$ is the data matrix with $m$ rows as the Tweets and $n$ columns representing each of the words in the Bag-of-Words. However, different methods may also be explored which change the weightings of the features.
\subsubsection{Results}

\subsection{The Hollow Matrix $\mathbf{A}_{h}$}
As described in Appendix A, the matrix $\mathbf{A} = \mathbf{S}^T \mathbf{S}$ can be viewed as an undirected graph with each node being a word and each link being the number of times that word appears with the connecting word node in the same Tweet for all Tweets. This representation is meaningful, however the problem lies in the fact that the diagonal basically is the number of times each word appears in total regardless of any relation to the other words i.e. each node has a link to itself, with the highest weighting. This means that words that occur very frequently have very large values on their diagonal, regardless of how they relate to other words, which can be deceiving when taking the eigenvalues of the matrix. In attempt to prevent this, the matrix $\mathbf{A}$ can be substituted for 
$\mathbf{A}_h$ where for each of its elements, $a'_{i, j}$, it takes the value 

\begin{equation}
a'_{i, j} = 
\begin{cases}
a_{i, j} & \text{if}\ i \neq j\\
0 & \text{if}\ i = j
\end{cases}
\end{equation}
where $a_{i, j}$ are the elements in $\mathbf{A}$.

\subsubsection{Results}

\subsection{The Normalised Matrix $\mathbf{A}_{n}$}
\subsubsection{Results}


\subsection{The Normalised Hollow Matrix $\mathbf{A}_{n}$}
\subsubsection{Results}
\section{Testing on Example Data}

\clearpage
\bibliography{bibliography}
\bibliographystyle{plain}
\clearpage

\section*{Appendix}
\pagenumbering{Roman}

\section*{A. Useful Intuition for the Sparse PCA Algorithm}
Consider a matrix, $S\in \mathbb{R}^{m\times n}$, where $m$ is the number of Tweets and $n$ is the number of features that the tweets are evaluated on, in this case the Bag-of-Words. Performing $S^TS = A$ where $A \in \mathbb{R}^{n\times n}$. 

By example, imagine:

\begin{equation}
S = \left( \begin{matrix}
a_{1, 1} & a_{1, 2} & a_{1, 3} \\ 
a_{2, 1} & a_{2, 2} & a_{2, 3} \\
a_{3, 1} & a_{3, 2} & a_{3, 3} \\
a_{4, 1} & a_{4, 2} & a_{4, 3} 
\end{matrix} \right)
\end{equation}

\begin{equation}
A = S^TS =
 \left( \begin{matrix}
a_{1, 1} & a_{2, 1} & a_{3, 1} & a_{4, 1} \\ 
a_{1, 2} & a_{2, 2} & a_{3, 2} & a_{4, 2} \\ 
a_{1, 3} & a_{2, 3} & a_{3, 3} & a_{4, 3} \\ 

\end{matrix} \right)
\cdot \left( \begin{matrix}
a_{1, 1} & a_{1, 2} & a_{1, 3} \\ 
a_{2, 1} & a_{2, 2} & a_{2, 3} \\
a_{3, 1} & a_{3, 2} & a_{3, 3} \\
a_{4, 1} & a_{4, 2} & a_{4, 3} 
\end{matrix} \right)
\end{equation}


\begin{equation}
A =
\left( \begin{matrix}
\sum_{i=1}^4 a_{i,1}^2& \sum_{i=1}^4 a_{i,1} a_{i,2} &  \sum_{i=1}^4 a_{i,1} a_{i,3} \\ 
\sum_{i=1}^4 a_{i,2}a_{i, 1} & \sum_{i=1}^4 a_{i,2}^2 &  \sum_{i=1}^4 a_{i,2} a_{i,3} \\ 
\sum_{i=1}^4 a_{i,3}^2& \sum_{i=1}^4 a_{i,3} a_{i,2} &  \sum_{i=1}^4  a_{i,3}^2 \\ 
\end{matrix} \right)
\end{equation}

Since $a_{i, j} = 1$ when the word $j$ appears in Tweet $i$ and otherwise 0, the resulting matrix is basically a symmetric matrix with entry $(i, j)$ being composed of the number of times word $j$ occurs with word $i$ in the same Tweet over all the Tweets. The diagonal is  therefore the number of times the word $j$ appears over all the Tweets.

This is basically a form of clustering i.e. words that occur frequently together will have high corresponding values in the matrix $A$.  
\end{document}
